{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "# torch.cuda.empty_cache()\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_pretrained_bert.optimization import BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_classifier import ColaProcessor, MrpcProcessor, logger, convert_examples_to_features,\\\n",
    "    set_optimizer_params_grad, copy_optimizer_params_to_model, accuracy, p_r_f1, tp_pcount_gcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    print('There are %d GPU(s) available.' % n_gpu)\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/08/2020 23:55:03 - INFO - transformers.file_utils -   PyTorch version 1.4.0 available.\n",
      "04/08/2020 23:55:36 - INFO - transformers.file_utils -   TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "def train_and_test(data_dir, bert_model=\"bert-base-uncased\", task_name=None,\n",
    "                   output_dir=None, max_seq_length=32, do_train=False, do_eval=False, do_lower_case=False,\n",
    "                   train_batch_size=32, eval_batch_size=8, learning_rate=5e-5, num_train_epochs=3,\n",
    "                   warmup_proportion=0.1,no_cuda=False, local_rank=-1, seed=42, gradient_accumulation_steps=1,\n",
    "                   optimize_on_cpu=False, fp16=False, loss_scale=128, saved_model=\"\"):\n",
    "\n",
    "\n",
    "    # ## Required parameters\n",
    "    # parser.add_argument(\"--data_dir\",\n",
    "    #                     default=None,\n",
    "    #                     type=str,\n",
    "    #                     required=True,\n",
    "    #                     help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "    # parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
    "    #                     help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "    #                          \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
    "    # parser.add_argument(\"--task_name\",\n",
    "    #                     default=None,\n",
    "    #                     type=str,\n",
    "    #                     required=True,\n",
    "    #                     help=\"The name of the task to train.\")\n",
    "    # parser.add_argument(\"--output_dir\",\n",
    "    #                     default=None,\n",
    "    #                     type=str,\n",
    "    #                     required=True,\n",
    "    #                     help=\"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "    ## Other parameters\n",
    "    # parser.add_argument(\"--max_seq_length\",\n",
    "    #                     default=128,\n",
    "    #                     type=int,\n",
    "    #                     help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "    #                          \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "    #                          \"than this will be padded.\")\n",
    "    # parser.add_argument(\"--do_train\",\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Whether to run training.\")\n",
    "    # parser.add_argument(\"--do_eval\",\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Whether to run eval on the dev set.\")\n",
    "    # parser.add_argument(\"--do_lower_case\",\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Set this flag if you are using an uncased model.\")\n",
    "    # parser.add_argument(\"--train_batch_size\",\n",
    "    #                     default=32,\n",
    "    #                     type=int,\n",
    "    #                     help=\"Total batch size for training.\")\n",
    "    # parser.add_argument(\"--eval_batch_size\",\n",
    "    #                     default=8,\n",
    "    #                     type=int,\n",
    "    #                     help=\"Total batch size for eval.\")\n",
    "    # parser.add_argument(\"--learning_rate\",\n",
    "    #                     default=5e-5,\n",
    "    #                     type=float,\n",
    "    #                     help=\"The initial learning rate for Adam.\")\n",
    "    # parser.add_argument(\"--num_train_epochs\",\n",
    "    #                     default=3.0,\n",
    "    #                     type=float,\n",
    "    #                     help=\"Total number of training epochs to perform.\")\n",
    "    # parser.add_argument(\"--warmup_proportion\",\n",
    "    #                     default=0.1,\n",
    "    #                     type=float,\n",
    "    #                     help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    #                          \"E.g., 0.1 = 10%% of training.\")\n",
    "    # parser.add_argument(\"--no_cuda\",\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Whether not to use CUDA when available\")\n",
    "    # parser.add_argument(\"--local_rank\",\n",
    "    #                     type=int,\n",
    "    #                     default=-1,\n",
    "    #                     help=\"local_rank for distributed training on gpus\")\n",
    "    # parser.add_argument('--seed',\n",
    "    #                     type=int,\n",
    "    #                     default=42,\n",
    "    #                     help=\"random seed for initialization\")\n",
    "    # parser.add_argument('--gradient_accumulation_steps',\n",
    "    #                     type=int,\n",
    "    #                     default=1,\n",
    "    #                     help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    # parser.add_argument('--optimize_on_cpu',\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Whether to perform optimization and keep the optimizer averages on CPU\")\n",
    "    # parser.add_argument('--fp16',\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "    # parser.add_argument('--loss_scale',\n",
    "    #                     type=float, default=128,\n",
    "    #                     help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    processors = {\n",
    "#         \"cola\": ColaProcessor,\n",
    "#         \"mnli\": MnliProcessor,\n",
    "        \"mrpc\": MrpcProcessor,\n",
    "    }\n",
    "\n",
    "    if local_rank == -1 or no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        if fp16:\n",
    "            logger.info(\"16-bits training currently not supported in distributed training\")\n",
    "            fp16 = False # (see https://github.com/pytorch/pytorch/pull/13496)\n",
    "    logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(local_rank != -1))\n",
    "\n",
    "    if gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            gradient_accumulation_steps))\n",
    "\n",
    "    train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if not do_train and not do_eval:\n",
    "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "    if do_train:\n",
    "        if os.path.exists(output_dir) and os.listdir(output_dir):\n",
    "            raise ValueError(\"Output directory ({}) already exists and is not emp1ty.\".format(output_dir))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    task_name = task_name.lower()\n",
    "\n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "    processor = processors[task_name]()\n",
    "    label_list = processor.get_labels()\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n",
    "\n",
    "    train_examples = None\n",
    "    num_train_steps = None\n",
    "    if do_train:\n",
    "        train_examples = processor.get_train_examples(data_dir)\n",
    "        num_train_steps = int(\n",
    "            len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)\n",
    "\n",
    "    # Prepare model\n",
    "    model = BertForSequenceClassification.from_pretrained(bert_model,\n",
    "                cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(local_rank), num_labels = 2)\n",
    "    if fp16:\n",
    "        model.half()\n",
    "    model.to(device)\n",
    "    if local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank],\n",
    "                                                          output_device=local_rank)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Prepare optimizer\n",
    "    if fp16:\n",
    "        param_optimizer = [(n, param.clone().detach().to('cpu').float().requires_grad_()) \\\n",
    "                            for n, param in model.named_parameters()]\n",
    "    elif optimize_on_cpu:\n",
    "        param_optimizer = [(n, param.clone().detach().to('cpu').requires_grad_()) \\\n",
    "                            for n, param in model.named_parameters()]\n",
    "    else:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    t_total = num_train_steps\n",
    "#     print(t_total)\n",
    "    if local_rank != -1:\n",
    "        t_total = t_total // torch.distributed.get_world_size()\n",
    "    if do_train:\n",
    "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=learning_rate,\n",
    "                         warmup=warmup_proportion,\n",
    "                         t_total=t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    if do_train:\n",
    "        train_features = convert_examples_to_features(\n",
    "            train_examples, label_list, max_seq_length, tokenizer)\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "        logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "        logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "        if local_rank == -1:\n",
    "            train_sampler = RandomSampler(train_data)\n",
    "        else:\n",
    "            train_sampler = DistributedSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "        model.train()\n",
    "        for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, segment_ids, label_ids = batch\n",
    "                loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                if n_gpu > 1:\n",
    "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
    "                if fp16 and loss_scale != 1.0:\n",
    "                    # rescale loss for fp16 training\n",
    "                    # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "                    loss = loss * loss_scale\n",
    "                if gradient_accumulation_steps > 1:\n",
    "                    loss = loss / gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += input_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "                if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                    if fp16 or optimize_on_cpu:\n",
    "                        if fp16 and loss_scale != 1.0:\n",
    "                            # scale down gradients for fp16 training\n",
    "                            for param in model.parameters():\n",
    "                                if param.grad is not None:\n",
    "                                    param.grad.data = param.grad.data / loss_scale\n",
    "                        is_nan = set_optimizer_params_grad(param_optimizer, model.named_parameters(), test_nan=True)\n",
    "                        if is_nan:\n",
    "                            logger.info(\"FP16 TRAINING: Nan in gradients, reducing loss scaling\")\n",
    "                            loss_scale = loss_scale / 2\n",
    "                            model.zero_grad()\n",
    "                            continue\n",
    "                        optimizer.step()\n",
    "                        copy_optimizer_params_to_model(model.named_parameters(), param_optimizer)\n",
    "                    else:\n",
    "                        optimizer.step()\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "        torch.save(model.state_dict(), output_dir + \"output.pth\")\n",
    "\n",
    "\n",
    "    if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        eval_examples = processor.get_test_examples(data_dir)\n",
    "#         eval_examples = processor.get_dev_examples(data_dir)\n",
    "        eval_features = convert_examples_to_features(\n",
    "            eval_examples, label_list, max_seq_length, tokenizer)\n",
    "        logger.info(\"***** Running evaluation *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "        logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "        # Run prediction for full data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "        model.load_state_dict(torch.load(saved_model))\n",
    "\n",
    "        model.eval()\n",
    "        # eval_loss, eval_accuracy = 0, 0\n",
    "\n",
    "        eval_tp, eval_pred_c, eval_gold_c = 0, 0, 0\n",
    "        eval_loss, eval_macro_p, eval_macro_r = 0, 0, 0\n",
    "\n",
    "        raw_score = []\n",
    "\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "#             print(logits)\n",
    "#             print(logits[0])\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "#             print(logits)\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "\n",
    "            # Micro F1 (aggregated tp, fp, fn counts across all examples)\n",
    "            tmp_tp, tmp_pred_c, tmp_gold_c = tp_pcount_gcount(logits, label_ids)\n",
    "            eval_tp += tmp_tp\n",
    "            eval_pred_c += tmp_pred_c\n",
    "            eval_gold_c += tmp_gold_c\n",
    "            \n",
    "            pred_label = np.argmax(logits, axis=1)\n",
    "            raw_score += zip(logits, pred_label, label_ids)\n",
    "            \n",
    "            # Macro F1 (averaged P, R across mini batches)\n",
    "            tmp_eval_p, tmp_eval_r, tmp_eval_f1 = p_r_f1(logits, label_ids)\n",
    "\n",
    "            eval_macro_p += tmp_eval_p\n",
    "            eval_macro_r += tmp_eval_r\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            nb_eval_examples += input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "\n",
    "        # Micro F1 (aggregated tp, fp, fn counts across all examples)\n",
    "        eval_micro_p = eval_tp / eval_pred_c\n",
    "        eval_micro_r = eval_tp / eval_gold_c\n",
    "        eval_micro_f1 = 2 * eval_micro_p * eval_micro_r / (eval_micro_p + eval_micro_r)\n",
    "\n",
    "        # Macro F1 (averaged P, R across mini batches)\n",
    "        eval_macro_p = eval_macro_p / nb_eval_steps\n",
    "        eval_macro_r = eval_macro_r / nb_eval_steps\n",
    "        eval_macro_f1 = 2 * eval_macro_p * eval_macro_r / (eval_macro_p + eval_macro_r)\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        result = {'eval_loss': eval_loss,\n",
    "                  'eval_micro_p': eval_micro_p,\n",
    "                  'eval_micro_r': eval_micro_r,\n",
    "                  'eval_micro_f1': eval_micro_f1,\n",
    "#                   'eval_macro_p': eval_macro_p,\n",
    "#                   'eval_macro_r': eval_macro_r,\n",
    "#                   'eval_macro_f1': eval_macro_f1,\n",
    "#                   'global_step': global_step,\n",
    "#                   'loss': tr_loss/nb_tr_steps\n",
    "                  }\n",
    "\n",
    "        output_eval_file = os.path.join(output_dir, \"opposite_eval_results.txt\")\n",
    "        output_raw_score = os.path.join(output_dir, \"opposite_raw_score.csv\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "        with open(output_raw_score, 'w') as fout:\n",
    "            fields = [\"undermine_score\", \"support_score\",\"predict_label\", \"gold\"]\n",
    "            writer = csv.DictWriter(fout, fieldnames=fields)\n",
    "            writer.writeheader()\n",
    "            for score, pred, gold in raw_score:\n",
    "                writer.writerow({\n",
    "                    \"undermine_score\": str(score[0]),\n",
    "                    \"support_score\": str(score[1]),\n",
    "                    \"predict_label\": str(pred),\n",
    "                    \"gold\": str(gold)\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments():\n",
    "    data_dir = \"D:/Jupyter/stance_code_local/failed_samples/\"\n",
    "#     data_dir = \"/home/syg340/Dataset/\"\n",
    "\n",
    "    # data_dir_output = data_dir + \"output2/\"\n",
    "    data_dir_output = \"D:/Projects/Stance/Models/\"\n",
    "    train_and_test(data_dir=data_dir, do_train=True, do_eval=True, output_dir=data_dir_output,task_name=\"Mrpc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_with_pretrained():\n",
    "    bert_model = \"D:/Projects/Stance/Models/output.pth\"\n",
    "    data_dir = \"D:/Jupyter/stance_code_local/failed_samples/\"\n",
    "    # data_dir_output = data_dir + \"output2/\"\n",
    "    data_dir_output = \"D:/Jupyter/stance_code_local/evaluation/bert_local_output/\"\n",
    "    train_and_test(data_dir=data_dir, do_train=False, do_eval=True, output_dir=data_dir_output,task_name=\"Mrpc\",saved_model=bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/08/2020 23:56:57 - INFO - run_classifier -   device cuda n_gpu 1 distributed training False\n",
      "04/08/2020 23:57:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\arsen\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "04/08/2020 23:57:07 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\arsen\\.pytorch_pretrained_bert\\distributed_-1\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "04/08/2020 23:57:07 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file C:\\Users\\arsen\\.pytorch_pretrained_bert\\distributed_-1\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\arsen\\AppData\\Local\\Temp\\tmpyi_bn9vd\n",
      "04/08/2020 23:57:11 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/08/2020 23:57:13 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/08/2020 23:57:13 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   *** Example ***\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   guid: test-1\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   tokens: [CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 18 [SEP] [UNK] is accomplished when education is forced . [SEP]\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   input_ids: 101 100 100 100 100 100 100 2324 102 100 2003 8885 2043 2495 2003 3140 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   label: 0 (id = 0)\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   *** Example ***\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   guid: test-2\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   tokens: [CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 18 [SEP] [UNK] education achieve ##s big [SEP]\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   input_ids: 101 100 100 100 100 100 100 2324 102 100 2495 6162 2015 2502 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   label: 0 (id = 0)\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   *** Example ***\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   guid: test-3\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   tokens: [CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 18 [SEP] [UNK] skills are best learnt in a classroom environment . [SEP]\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   input_ids: 101 100 100 100 100 100 100 2324 102 100 4813 2024 2190 20215 1999 1037 9823 4044 1012 102 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   label: 0 (id = 0)\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   *** Example ***\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   guid: test-4\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   tokens: [CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 18 [SEP] [UNK] skills are worse learned inside the classroom . [SEP]\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   input_ids: 101 100 100 100 100 100 100 2324 102 100 4813 2024 4788 4342 2503 1996 9823 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   label: 0 (id = 0)\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   *** Example ***\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   guid: test-5\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   tokens: [CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 18 [SEP] [UNK] people should not have the choice to decide for themselves [SEP]\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   input_ids: 101 100 100 100 100 100 100 2324 102 100 2111 2323 2025 2031 1996 3601 2000 5630 2005 3209 102 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   label: 0 (id = 0)\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -   ***** Running evaluation *****\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -     Num examples = 98\n",
      "04/08/2020 23:57:20 - INFO - run_classifier -     Batch size = 8\n",
      "C:\\Users\\arsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:319: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "04/08/2020 23:57:28 - INFO - run_classifier -   ***** Eval results *****\n",
      "04/08/2020 23:57:28 - INFO - run_classifier -     eval_loss = 1.448869792314676\n",
      "04/08/2020 23:57:28 - INFO - run_classifier -     eval_micro_f1 = nan\n",
      "04/08/2020 23:57:28 - INFO - run_classifier -     eval_micro_p = 0.0\n",
      "04/08/2020 23:57:28 - INFO - run_classifier -     eval_micro_r = nan\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     experiments()\n",
    "    evaluation_with_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

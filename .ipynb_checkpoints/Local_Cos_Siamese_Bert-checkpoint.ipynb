{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# torch.cuda.empty_cache()\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_pretrained_bert.optimization import BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_classifier import StanceProcessor, MrpcProcessor, logger, convert_examples_to_features,\\\n",
    "    set_optimizer_params_grad, copy_optimizer_params_to_model, accuracy, p_r_f1, tp_pcount_gcount, convert_claims_to_features, convert_pers_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/11/2020 10:29:05 - INFO - run_classifier -   There are 1 GPU(s) available.\n",
      "06/11/2020 10:29:05 - INFO - run_classifier -   We will use the GPU:\n",
      "06/11/2020 10:29:05 - INFO - run_classifier -   GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info('There are %d GPU(s) available.' % (n_gpu))\n",
    "    logger.info('We will use the GPU:')\n",
    "    logger.info(torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    logger.info('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertPreTrainedModel, BertModel, BertConfig\n",
    "from torch.nn import BCEWithLogitsLoss, CosineEmbeddingLoss,CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForConsistencyCueClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForConsistencyCueClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size*4+1, num_labels)\n",
    "        self.classifier2 = torch.nn.Linear(config.hidden_size*4, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "#         self.init_weights()\n",
    "\n",
    "#     @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        input_ids2=None,\n",
    "        attention_mask2=None,\n",
    "        token_type_ids2=None,\n",
    "        position_ids2=None,\n",
    "        head_mask2=None,\n",
    "        inputs_embeds2=None,\n",
    "        labels2=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
    "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "\n",
    "    Returns:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        import torch\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        _, outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "#             position_ids=position_ids,\n",
    "#             head_mask=head_mask,\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        _, outputs2 = self.bert(\n",
    "            input_ids2,\n",
    "            attention_mask=attention_mask2,\n",
    "            token_type_ids=token_type_ids2,\n",
    "#             position_ids=position_ids2,\n",
    "#             head_mask=head_mask2,\n",
    "#             inputs_embeds=inputs_embeds2,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs\n",
    "        pooled_output2 = outputs2\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        pooled_output2 = self.dropout(pooled_output2)\n",
    "        \n",
    "#         A series of different concatenations(concat(),|minus|,multiply, ...)\n",
    "        final_output_cat = torch.cat((pooled_output, pooled_output2),1)\n",
    "        final_output_minus = torch.abs(pooled_output-pooled_output2)\n",
    "        final_output_mult = torch.mul(pooled_output, pooled_output2)\n",
    "#         final_output_mimu = torch.cat((final_output_minus, final_output_mult),1)\n",
    "#         final_output_camu = torch.cat((final_output_cat, final_output_mult),1)\n",
    "#         final_output_cami = torch.cat((final_output_cat, final_output_minus),1)\n",
    "        final_output_camimu = torch.cat((final_output_cat, final_output_minus, final_output_mult),1)\n",
    "    \n",
    "        cos_pooled_outputs = torch.cosine_similarity(pooled_output, pooled_output2, dim=1)\n",
    "#         1\n",
    "#         torch.Size([hidden_size*2, 768])\n",
    "#         2\n",
    "#         torch.Size([hidden_size, 768])\n",
    "#         3\n",
    "#         torch.Size([hidden_size, 768])\n",
    "#         4\n",
    "#         torch.Size([hidden_size*2, 768])\n",
    "#         5\n",
    "#         torch.Size([hidden_size*3, 768])\n",
    "#         6\n",
    "#         torch.Size([hidden_size*3, 768])\n",
    "#         7\n",
    "#         torch.Size([hidden_size*4, 768])\n",
    "        \n",
    "        batch_size = list(pooled_output.size())[0]\n",
    "        hidden_size = list(pooled_output.size())[1]\n",
    "        \n",
    "        final_output_all = torch.cat((final_output_camimu, cos_pooled_outputs.unsqueeze(1)),1)\n",
    "        logits_ce = self.classifier(final_output_all)\n",
    "#         print('logits_ce:')\n",
    "#         print(logits_ce)\n",
    "        \n",
    "#         logits_ori = self.classifier2(final_output_camimu)\n",
    "#         print('logits_ori:')\n",
    "#         print(logits_ori)\n",
    "\n",
    "        #Calculate loss during training process\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct_ce = CrossEntropyLoss()\n",
    "                loss_ce = loss_fct_ce(logits_ce.view(-1, self.num_labels), labels.view(-1))\n",
    "                print('loss_ce:')\n",
    "                print(loss_ce)\n",
    "\n",
    "#                 loss_ori = loss_fct_ce(logits_ori.view(-1, self.num_labels), labels.view(-1))\n",
    "#                 print('loss_ori:')\n",
    "#                 print(loss_ori)\n",
    "                loss_fct_cos = CosineEmbeddingLoss()\n",
    "\n",
    "                labels2[labels2==0] = -1\n",
    "                loss_cos = loss_fct_cos(pooled_output, pooled_output2, labels2)\n",
    "                labels2[labels2==-1] = 0\n",
    "                print('loss_cos:')\n",
    "                print(loss_cos)\n",
    "            \n",
    "                loss = loss_ce\n",
    "                print('final loss:')\n",
    "                print(loss)\n",
    "                \n",
    "#             outputs = (loss,) + outputs\n",
    "#             outputs = (loss,) + logits_cos \n",
    "                outputs = loss\n",
    "                return outputs\n",
    "        else:\n",
    "            #Get predictions when doing evaluation\n",
    "            return logits_ce\n",
    "        \n",
    "          # (loss), logits, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all of the model's parameters as a list of tuples.\n",
    "# params = list(model.named_parameters())\n",
    "\n",
    "# print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "# print('==== Embedding Layer ====\\n')\n",
    "\n",
    "# for p in params[0:5]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "# print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "# for p in params[5:21]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    \n",
    "# print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "# for p in params[-4:]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"D:/Jupyter/data/dataset/perspective_stances/\"\n",
    "# # data_dir = \"D:/Projects/Stance/Dataset/OnlyNew/\"\n",
    "# data_dir_output = \"D:/Projects/Stance/Models/Consistency_Cues/\"\n",
    "# output_dir=data_dir_output\n",
    "# max_seq_length=32\n",
    "# max_grad_norm = 1.0\n",
    "# num_training_steps = 1000\n",
    "# num_warmup_steps = 100\n",
    "# warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n",
    "# # warmup_proportion = 0.1\n",
    "# # train_batch_size=32\n",
    "# train_batch_size=16\n",
    "# eval_batch_size=8\n",
    "# learning_rate=5e-5\n",
    "# num_train_epochs=4\n",
    "# local_rank=-1\n",
    "# seed=19\n",
    "# gradient_accumulation_steps=1\n",
    "# loss_scale=128\n",
    "# train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n",
    "\n",
    "# processors = {\n",
    "#         \"mrpc\": MrpcProcessor,\n",
    "#     }\n",
    "\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# processor = processors['mrpc']()\n",
    "# label_list = processor.get_labels()\n",
    "# num_labels = len(label_list)\n",
    "# # print('label list')\n",
    "# # print(label_list)\n",
    "\n",
    "# train_examples = processor.get_train_examples(data_dir)\n",
    "# num_train_steps = int(\n",
    "#     len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)\n",
    "\n",
    "# ##preprare optimizer\n",
    "# param_optimizer = list(model.named_parameters())\n",
    "# no_decay = ['bias', 'gamma', 'beta']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "#     ]\n",
    "# t_total = num_train_steps\n",
    "# optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "#                          lr=learning_rate,\n",
    "#                          warmup=warmup_proportion,\n",
    "#                          t_total=t_total)\n",
    "# # optimizer = AdamW(optimizer_grouped_parameters,\n",
    "# #                   lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "# #                   eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
    "# #                   correct_bias=False\n",
    "# #                 )\n",
    "\n",
    "# # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_step = 0\n",
    "# claim_features = convert_claims_to_features(train_examples, label_list, max_seq_length, tokenizer)\n",
    "# train_features = convert_pers_to_features(train_examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "# logger.info(\"***** Running training *****\")\n",
    "# logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "# logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "# logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "\n",
    "# all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "# all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "# all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "# all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "\n",
    "# claims_input_ids = torch.tensor([f.input_ids for f in claim_features], dtype=torch.long)\n",
    "# claims_input_mask = torch.tensor([f.input_mask for f in claim_features], dtype=torch.long)\n",
    "# claims_segment_ids = torch.tensor([f.segment_ids for f in claim_features], dtype=torch.long)\n",
    "# claims_label_ids = torch.tensor([f.label_id for f in claim_features], dtype=torch.long)\n",
    "\n",
    "# train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, claims_input_ids, claims_input_mask, claims_segment_ids, claims_label_ids)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "# # claims_data = TensorDataset(claims_input_ids, claims_input_mask, claims_segment_ids, claims_label_ids)\n",
    "# # claims_sampler = RandomSampler(claims_data)\n",
    "# # claims_dataloader = DataLoader(claims_data, sampler=train_sampler, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "#     tr_loss = 0\n",
    "#     nb_tr_examples, nb_tr_steps = 0, 0\n",
    "#     for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         input_ids, input_mask, segment_ids, label_ids, claim_input_ids, claim_input_mask, claim_segment_ids, claim_label_ids = batch\n",
    "# #         ce_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "# #         cos_loss = model(claim_input_ids, claim_segment_ids, claim_input_mask, claim_label_ids)\n",
    "        \n",
    "# #         print(\"start\")\n",
    "# #         print(input_ids)\n",
    "# #         print(input_mask)\n",
    "# #         print(segment_ids)\n",
    "# #         print(label_ids)\n",
    "# #         print(claim_input_ids)\n",
    "# #         print(claim_input_mask)\n",
    "# #         print(claim_segment_ids)\n",
    "# #         print(claim_label_ids)\n",
    "# #         print(\"end\")\n",
    "    \n",
    "#         out_results = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, labels=label_ids, input_ids2=claim_input_ids, token_type_ids2=claim_segment_ids, attention_mask2=claim_input_mask, labels2=claim_label_ids)\n",
    "# #         loss = ce_loss + cos_loss\n",
    "#         print(\"out_results:\")\n",
    "#         print(out_results)\n",
    "#         loss = out_results\n",
    "# #         print(cos_loss)\n",
    "# #         print(loss.item())\n",
    "#         if n_gpu > 1:\n",
    "#             loss = loss.mean() # mean() to average on multi-gpu.\n",
    "# #         if fp16 and loss_scale != 1.0:\n",
    "# #             # rescale loss for fp16 training\n",
    "# #             # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "# #             loss = loss * loss_scale\n",
    "#         if gradient_accumulation_steps > 1:\n",
    "#             loss = loss / gradient_accumulation_steps\n",
    "#         loss.backward()\n",
    "        \n",
    "#         tr_loss += loss.item()\n",
    "#         nb_tr_examples += input_ids.size(0)\n",
    "#         nb_tr_steps += 1\n",
    "#         if (step + 1) % gradient_accumulation_steps == 0:\n",
    "# #             if fp16 or optimize_on_cpu:\n",
    "# #                 if fp16 and loss_scale != 1.0:\n",
    "# #                     # scale down gradients for fp16 training\n",
    "# #                     for param in model.parameters():\n",
    "# #                         if param.grad is not None:\n",
    "# #                             param.grad.data = param.grad.data / loss_scale           \n",
    "# #                 is_nan = set_optimizer_params_grad(param_optimizer, model.named_parameters(), test_nan=True)\n",
    "# #                 if is_nan:\n",
    "# #                     logger.info(\"FP16 TRAINING: Nan in gradients, reducing loss scaling\")\n",
    "# #                     loss_scale = loss_scale / 2\n",
    "# #                     model.zero_grad()\n",
    "# #                     continue \n",
    "# #                 optimizer.step()\n",
    "# # #                 scheduler.step()\n",
    "# #                 copy_optimizer_params_to_model(model.named_parameters(), param_optimizer)\n",
    "# #             else:\n",
    "# #                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#             optimizer.step()\n",
    "# #                 scheduler.step()\n",
    "#             model.zero_grad()\n",
    "#             global_step += 1\n",
    "\n",
    "        \n",
    "# ## v2: concat\n",
    "# ## v3: multiply\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model\n",
    "# torch.save(model.state_dict(), output_dir + \"cosloss_camimu_siamese_bert_epoch4.pth\")\n",
    "# # torch.save(model_to_save.state_dict(), output_dir + \"cos_camimu_siamese_bert.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "def train_and_test(data_dir, bert_model=\"bert-base-uncased\", task_name=None,\n",
    "                   output_dir=None, max_seq_length=32, do_train=False, do_eval=False, do_lower_case=False,\n",
    "                   train_batch_size=16, eval_batch_size=8, learning_rate=5e-5, num_train_epochs=5,\n",
    "                   warmup_proportion=0.1,no_cuda=False, local_rank=-1, seed=19, gradient_accumulation_steps=1,\n",
    "                   optimize_on_cpu=False, fp16=False, loss_scale=128, saved_model=\"\"):\n",
    "    \n",
    "\n",
    "\n",
    "    # ## Required parameters\n",
    "    # parser.add_argument(\"--data_dir\",\n",
    "    #                     default=None,\n",
    "    #                     type=str,\n",
    "    #                     required=True,\n",
    "    #                     help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "    # parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
    "    #                     help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "    #                          \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
    "    # parser.add_argument(\"--task_name\",\n",
    "    #                     default=None,\n",
    "    #                     type=str,\n",
    "    #                     required=True,\n",
    "    #                     help=\"The name of the task to train.\")\n",
    "    # parser.add_argument(\"--output_dir\",\n",
    "    #                     default=None,\n",
    "    #                     type=str,\n",
    "    #                     required=True,\n",
    "    #                     help=\"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "    ## Other parameters\n",
    "    # parser.add_argument(\"--max_seq_length\",\n",
    "    #                     default=128,\n",
    "    #                     type=int,\n",
    "    #                     help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "    #                          \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "    #                          \"than this will be padded.\")\n",
    "    # parser.add_argument(\"--do_train\",\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Whether to run training.\")\n",
    "    # parser.add_argument(\"--do_eval\",\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Whether to run eval on the dev set.\")\n",
    "    # parser.add_argument(\"--do_lower_case\",\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Set this flag if you are using an uncased model.\")\n",
    "    # parser.add_argument(\"--train_batch_size\",\n",
    "    #                     default=32,\n",
    "    #                     type=int,\n",
    "    #                     help=\"Total batch size for training.\")\n",
    "    # parser.add_argument(\"--eval_batch_size\",\n",
    "    #                     default=8,\n",
    "    #                     type=int,\n",
    "    #                     help=\"Total batch size for eval.\")\n",
    "    # parser.add_argument(\"--learning_rate\",\n",
    "    #                     default=5e-5,\n",
    "    #                     type=float,\n",
    "    #                     help=\"The initial learning rate for Adam.\")\n",
    "    # parser.add_argument(\"--num_train_epochs\",\n",
    "    #                     default=3.0,\n",
    "    #                     type=float,\n",
    "    #                     help=\"Total number of training epochs to perform.\")\n",
    "    # parser.add_argument(\"--warmup_proportion\",\n",
    "    #                     default=0.1,\n",
    "    #                     type=float,\n",
    "    #                     help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    #                          \"E.g., 0.1 = 10%% of training.\")\n",
    "    # parser.add_argument(\"--no_cuda\",\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Whether not to use CUDA when available\")\n",
    "    # parser.add_argument(\"--local_rank\",\n",
    "    #                     type=int,\n",
    "    #                     default=-1,\n",
    "    #                     help=\"local_rank for distributed training on gpus\")\n",
    "    # parser.add_argument('--seed',\n",
    "    #                     type=int,\n",
    "    #                     default=42,\n",
    "    #                     help=\"random seed for initialization\")\n",
    "    # parser.add_argument('--gradient_accumulation_steps',\n",
    "    #                     type=int,\n",
    "    #                     default=1,\n",
    "    #                     help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    # parser.add_argument('--optimize_on_cpu',\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Whether to perform optimization and keep the optimizer averages on CPU\")\n",
    "    # parser.add_argument('--fp16',\n",
    "    #                     default=False,\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "    # parser.add_argument('--loss_scale',\n",
    "    #                     type=float, default=128,\n",
    "    #                     help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    \n",
    "    processors = {\n",
    "#         \"cola\": ColaProcessor,\n",
    "#         \"mnli\": MnliProcessor,\n",
    "        \"mrpc\": MrpcProcessor,\n",
    "    }\n",
    "\n",
    "    if local_rank == -1 or no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        if fp16:\n",
    "            logger.info(\"16-bits training currently not supported in distributed training\")\n",
    "            fp16 = False # (see https://github.com/pytorch/pytorch/pull/13496)\n",
    "    logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(local_rank != -1))\n",
    "\n",
    "    if gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            gradient_accumulation_steps))\n",
    "\n",
    "    train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if not do_train and not do_eval:\n",
    "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "    if do_train:\n",
    "        if os.path.exists(output_dir) and os.listdir(output_dir):\n",
    "            raise ValueError(\"Output directory ({}) already exists and is not emp1ty.\".format(output_dir))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    task_name = task_name.lower()\n",
    "\n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "    processor = processors[task_name]()\n",
    "    label_list = processor.get_labels()\n",
    "\n",
    "#     tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "\n",
    "    train_examples = None\n",
    "    num_train_steps = None\n",
    "    if do_train:\n",
    "        train_examples = processor.get_train_examples(data_dir)\n",
    "        \n",
    "        num_train_steps = int(\n",
    "            len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)\n",
    "\n",
    "    # Prepare model\n",
    "#     model = BertForSequenceClassification.from_pretrained(bert_model,\n",
    "#                 cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(local_rank), num_labels = 2)\n",
    "\n",
    "        model = BertForConsistencyCueClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "        model.to(device)\n",
    "        \n",
    "        if fp16:\n",
    "            model.half()\n",
    "\n",
    "        if local_rank != -1:\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank],\n",
    "                                                              output_device=local_rank)\n",
    "        elif n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Prepare optimizer\n",
    "        if fp16:\n",
    "            param_optimizer = [(n, param.clone().detach().to('cpu').float().requires_grad_()) \\\n",
    "                                for n, param in model.named_parameters()]\n",
    "        elif optimize_on_cpu:\n",
    "            param_optimizer = [(n, param.clone().detach().to('cpu').requires_grad_()) \\\n",
    "                                for n, param in model.named_parameters()]\n",
    "        else:\n",
    "            param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "            ]\n",
    "        t_total = num_train_steps\n",
    "#     print(t_total)\n",
    "    if local_rank != -1:\n",
    "        t_total = t_total // torch.distributed.get_world_size()\n",
    "    if do_train:\n",
    "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=learning_rate,\n",
    "                         warmup=warmup_proportion,\n",
    "                         t_total=t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    if do_train:\n",
    "        claim_features = convert_claims_to_features(train_examples, label_list, max_seq_length, tokenizer)\n",
    "        train_features = convert_pers_to_features(train_examples, label_list, max_seq_length, tokenizer)\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "        logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "        logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "\n",
    "        claims_input_ids = torch.tensor([f.input_ids for f in claim_features], dtype=torch.long)\n",
    "        claims_input_mask = torch.tensor([f.input_mask for f in claim_features], dtype=torch.long)\n",
    "        claims_segment_ids = torch.tensor([f.segment_ids for f in claim_features], dtype=torch.long)\n",
    "        claims_label_ids = torch.tensor([f.label_id for f in claim_features], dtype=torch.long)\n",
    "\n",
    "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, claims_input_ids, claims_input_mask, claims_segment_ids, claims_label_ids)\n",
    "\n",
    "        if local_rank == -1:\n",
    "            train_sampler = RandomSampler(train_data)\n",
    "        else:\n",
    "            train_sampler = DistributedSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "        model.train()\n",
    "        for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, segment_ids, label_ids, claim_input_ids, claim_input_mask, claim_segment_ids, claim_label_ids = batch\n",
    "                \n",
    "                out_results = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, labels=label_ids, input_ids2=claim_input_ids, token_type_ids2=claim_segment_ids, attention_mask2=claim_input_mask, labels2=claim_label_ids)\n",
    "#                 loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                print(\"out_results:\")\n",
    "                print(out_results)\n",
    "                loss = out_results\n",
    "            \n",
    "                if n_gpu > 1:\n",
    "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
    "                if fp16 and loss_scale != 1.0:\n",
    "                    # rescale loss for fp16 training\n",
    "                    # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "                    loss = loss * loss_scale\n",
    "                if gradient_accumulation_steps > 1:\n",
    "                    loss = loss / gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += input_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "                if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                    if fp16 or optimize_on_cpu:\n",
    "                        if fp16 and loss_scale != 1.0:\n",
    "                            # scale down gradients for fp16 training\n",
    "                            for param in model.parameters():\n",
    "                                if param.grad is not None:\n",
    "                                    param.grad.data = param.grad.data / loss_scale\n",
    "                        is_nan = set_optimizer_params_grad(param_optimizer, model.named_parameters(), test_nan=True)\n",
    "                        if is_nan:\n",
    "                            logger.info(\"FP16 TRAINING: Nan in gradients, reducing loss scaling\")\n",
    "                            loss_scale = loss_scale / 2\n",
    "                            model.zero_grad()\n",
    "                            continue\n",
    "                        optimizer.step()\n",
    "                        copy_optimizer_params_to_model(model.named_parameters(), param_optimizer)\n",
    "                    else:\n",
    "                        optimizer.step()\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "        torch.save(model.state_dict(), output_dir + \"cos_camimu_siamese_bert_epoch5.pth\")\n",
    "\n",
    "\n",
    "    if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "#         eval_examples = processor.get_test_examples(data_dir)\n",
    "        eval_examples = processor.get_dev_examples(data_dir)\n",
    "        claim_features = convert_claims_to_features(eval_examples, label_list, max_seq_length, tokenizer)\n",
    "        eval_features = convert_pers_to_features(\n",
    "            eval_examples, label_list, max_seq_length, tokenizer)\n",
    "            \n",
    "    \n",
    "        logger.info(\"***** Running evaluation *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "        logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "        \n",
    "        claims_input_ids = torch.tensor([f.input_ids for f in claim_features], dtype=torch.long)\n",
    "        claims_input_mask = torch.tensor([f.input_mask for f in claim_features], dtype=torch.long)\n",
    "        claims_segment_ids = torch.tensor([f.segment_ids for f in claim_features], dtype=torch.long)\n",
    "        claims_label_ids = torch.tensor([f.label_id for f in claim_features], dtype=torch.long)\n",
    "        \n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, claims_input_ids, claims_input_mask, claims_segment_ids, claims_label_ids)\n",
    "        # Run prediction for full data\n",
    "#         eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "#         print('all_input_ids:')\n",
    "#         print(all_input_ids)\n",
    "        \n",
    "        \n",
    "\n",
    "#         model.load_state_dict(torch.load(saved_model))\n",
    "        model_state_dict = torch.load(saved_model)\n",
    "        model = BertForConsistencyCueClassification.from_pretrained('bert-base-uncased', num_labels=2, state_dict=model_state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        # eval_loss, eval_accuracy = 0, 0\n",
    "\n",
    "        eval_tp, eval_pred_c, eval_gold_c = 0, 0, 0\n",
    "        eval_loss, eval_macro_p, eval_macro_r = 0, 0, 0\n",
    "\n",
    "        raw_score = []\n",
    "\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        for input_ids, input_mask, segment_ids, label_ids, claim_input_ids, claim_input_mask, claim_segment_ids, claim_label_ids in eval_dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "            claim_input_ids = claim_input_ids.to(device)\n",
    "            claim_input_mask = claim_input_mask.to(device)\n",
    "            claim_segment_ids = claim_segment_ids.to(device)\n",
    "            claim_label_ids = claim_label_ids.to(device)\n",
    "\n",
    "#             print(\"start\")\n",
    "#             print(input_ids)\n",
    "#             print(input_mask)\n",
    "#             print(segment_ids)\n",
    "#             print(label_ids)\n",
    "#             print(claim_input_ids)\n",
    "#             print(claim_input_mask)\n",
    "#             print(claim_segment_ids)\n",
    "#             print(claim_label_ids)\n",
    "#             print(\"end\")\n",
    "            with torch.no_grad():\n",
    "                tmp_eval_loss = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, labels=label_ids, input_ids2=claim_input_ids, token_type_ids2=claim_segment_ids, attention_mask2=claim_input_mask, labels2=claim_label_ids)\n",
    "                \n",
    "                logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, input_ids2=claim_input_ids, token_type_ids2=claim_segment_ids, attention_mask2=claim_input_mask)\n",
    "            \n",
    "            print(logits)\n",
    "#             print(logits[0])\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            print(logits)\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "#             print(label_ids)\n",
    "\n",
    "            # Micro F1 (aggregated tp, fp, fn counts across all examples)\n",
    "            tmp_tp, tmp_pred_c, tmp_gold_c = tp_pcount_gcount(logits, label_ids)\n",
    "            eval_tp += tmp_tp\n",
    "            eval_pred_c += tmp_pred_c\n",
    "            eval_gold_c += tmp_gold_c\n",
    "            \n",
    "            pred_label = np.argmax(logits, axis=1)\n",
    "            raw_score += zip(logits, pred_label, label_ids)\n",
    "            \n",
    "            # Macro F1 (averaged P, R across mini batches)\n",
    "            tmp_eval_p, tmp_eval_r, tmp_eval_f1 = p_r_f1(logits, label_ids)\n",
    "\n",
    "            eval_macro_p += tmp_eval_p\n",
    "            eval_macro_r += tmp_eval_r\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            nb_eval_examples += input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "\n",
    "        # Micro F1 (aggregated tp, fp, fn counts across all examples)\n",
    "        eval_micro_p = eval_tp / eval_pred_c\n",
    "        eval_micro_r = eval_tp / eval_gold_c\n",
    "        eval_micro_f1 = 2 * eval_micro_p * eval_micro_r / (eval_micro_p + eval_micro_r)\n",
    "\n",
    "        # Macro F1 (averaged P, R across mini batches)\n",
    "        eval_macro_p = eval_macro_p / nb_eval_steps\n",
    "        eval_macro_r = eval_macro_r / nb_eval_steps\n",
    "        eval_macro_f1 = 2 * eval_macro_p * eval_macro_r / (eval_macro_p + eval_macro_r)\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        result = {\n",
    "                  'eval_loss': eval_loss,\n",
    "                  'eval_micro_p': eval_micro_p,\n",
    "                  'eval_micro_r': eval_micro_r,\n",
    "                  'eval_micro_f1': eval_micro_f1,\n",
    "                  'eval_macro_p': eval_macro_p,\n",
    "                  'eval_macro_r': eval_macro_r,\n",
    "                  'eval_macro_f1': eval_macro_f1,\n",
    "#                   'global_step': global_step,\n",
    "#                   'loss': tr_loss/nb_tr_steps\n",
    "                  }\n",
    "\n",
    "        output_eval_file = os.path.join(output_dir, \"cos_camimu_siamese_bert_epoch5_siamese_bert_test_eval_results.txt\")\n",
    "        output_raw_score = os.path.join(output_dir, \"cos_camimu_siamese_bert_epoch5_siamese_bert_test_raw_score.csv\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "        with open(output_raw_score, 'w') as fout:\n",
    "            fields = [\"undermine_score\", \"support_score\",\"predict_label\", \"gold\"]\n",
    "            writer = csv.DictWriter(fout, fieldnames=fields)\n",
    "            writer.writeheader()\n",
    "            for score, pred, gold in raw_score:\n",
    "                writer.writerow({\n",
    "                    \"undermine_score\": str(score[0]),\n",
    "                    \"support_score\": str(score[1]),\n",
    "                    \"predict_label\": str(pred),\n",
    "                    \"gold\": str(gold)\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments():\n",
    "    data_dir = \"D:/Jupyter/data/dataset/perspective_stances/\"\n",
    "    \n",
    "    data_dir_output = \"D:/Projects/Stance/Models/Siamese_bert/\"\n",
    "    train_and_test(data_dir=data_dir, do_train=True, do_eval=True, output_dir=data_dir_output,task_name=\"stance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_with_pretrained():\n",
    "    bert_model = \"D:/Projects/Stance/Models/Consistency_Cues/cos_camimu_siamese_bert_epoch5.pth\"\n",
    "    data_dir = \"D:/Jupyter/data/dataset/perspective_stances/\"\n",
    "    \n",
    "    data_dir_output = \"D:/Projects/Stance/Evaluation/local_output/\"\n",
    "    train_and_test(data_dir=data_dir, do_train=False, do_eval=True, output_dir=data_dir_output,task_name=\"stance\",saved_model=bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/11/2020 10:29:05 - INFO - run_classifier -   device cuda n_gpu 1 distributed training False\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Task not found: stance",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6319bb79c482>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mexperiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#     evaluation_with_pretrained()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-9414cc60f5a5>\u001b[0m in \u001b[0;36mexperiments\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata_dir_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"D:/Projects/Stance/Models/Siamese_bert/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain_and_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dir_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"stance\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-46cdafee2145>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[1;34m(data_dir, bert_model, task_name, output_dir, max_seq_length, do_train, do_eval, do_lower_case, train_batch_size, eval_batch_size, learning_rate, num_train_epochs, warmup_proportion, no_cuda, local_rank, seed, gradient_accumulation_steps, optimize_on_cpu, fp16, loss_scale, saved_model)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtask_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Task not found: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Task not found: stance"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    experiments()\n",
    "#     evaluation_with_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

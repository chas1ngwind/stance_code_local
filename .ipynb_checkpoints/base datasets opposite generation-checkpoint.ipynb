{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/ARC/arc_stances_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/ARC/arc_stances_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/FNC-1/competition_test_stances.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/FNC-1/train_stances.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/SemEval2016Task6/testdata-taskA-all-annotations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/SemEval2016Task6/trainingdata-all-annotations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"D:/Jupyter/data/dataset/ibmcs/test_ori.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"D:/Jupyter/data/dataset/ibmcs/test_train.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"D:/Jupyter/stance_code/Dataset/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(test_file)\n",
    "# df = pd.read_table(test_file,encoding='ISO-8859-1')\n",
    "df = pd.read_csv(test_file,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>claim</th>\n",
       "      <th>perspective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School Day Should Be Extended</td>\n",
       "      <td>Lessons would feel less broken.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School Day Should Be Extended</td>\n",
       "      <td>Not enough fundng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School Day Should Be Extended</td>\n",
       "      <td>So much easier for parents!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School Day Should Be Extended</td>\n",
       "      <td>Less need for homework.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School Day Should Be Extended</td>\n",
       "      <td>Time to finish activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Illegal downloaders should be cut off from the...</td>\n",
       "      <td>Cutting off internet capability is against hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Illegal downloaders should be cut off from the...</td>\n",
       "      <td>All humans deserve internet access.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Illegal downloaders should be cut off from the...</td>\n",
       "      <td>Internet access is a commodity not a human right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Illegal downloaders should be cut off from the...</td>\n",
       "      <td>The internet is a utility not a human right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Illegal downloaders should be cut off from the...</td>\n",
       "      <td>Just as electricity is not a human right, the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2773 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label  x  y                                              claim  \\\n",
       "0         1  0  0                      School Day Should Be Extended   \n",
       "1         0  0  0                      School Day Should Be Extended   \n",
       "2         1  0  0                      School Day Should Be Extended   \n",
       "3         1  0  0                      School Day Should Be Extended   \n",
       "4         1  0  0                      School Day Should Be Extended   \n",
       "...     ... .. ..                                                ...   \n",
       "2768      0  0  0  Illegal downloaders should be cut off from the...   \n",
       "2769      0  0  0  Illegal downloaders should be cut off from the...   \n",
       "2770      1  0  0  Illegal downloaders should be cut off from the...   \n",
       "2771      1  0  0  Illegal downloaders should be cut off from the...   \n",
       "2772      1  0  0  Illegal downloaders should be cut off from the...   \n",
       "\n",
       "                                            perspective  \n",
       "0                       Lessons would feel less broken.  \n",
       "1                                  Not enough fundng...  \n",
       "2                           So much easier for parents!  \n",
       "3                               Less need for homework.  \n",
       "4                             Time to finish activities  \n",
       "...                                                 ...  \n",
       "2768  Cutting off internet capability is against hum...  \n",
       "2769                All humans deserve internet access.  \n",
       "2770  Internet access is a commodity not a human right.  \n",
       "2771       The internet is a utility not a human right.  \n",
       "2772  Just as electricity is not a human right, the ...  \n",
       "\n",
       "[2773 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_toprocess = df.Headline\n",
    "column_toprocess = df.perspective\n",
    "# column_toprocess = df.claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fucntion to auto create opposite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"lower\")\n",
    "matcher.add(\"not\", None, nlp(\"not\"), nlp(\"n't\"))\n",
    "matcher.add(\"can't\", None, nlp(\"can't\"))\n",
    "\n",
    "\n",
    "matcher_positive = PhraseMatcher(nlp.vocab, attr=\"ORTH\")\n",
    "matcher_positive.add(\"type\", None, nlp(\"is a type of\"), nlp(\"are a type of\") )\n",
    "matcher_positive.add(\"imply\", None, nlp(\"implies\"), nlp(\"imply\") )\n",
    "matcher_positive.add(\"same\", None, nlp(\"is the same as \"), nlp(\"are the same as \") )\n",
    "matcher_positive.add(\"rephrase\", None, nlp(\" is a rephrasing of\") )\n",
    "matcher_positive.add(\"form\", None, nlp(\"is a another form of\"))\n",
    "matcher_positive.add(\"synonym\", None, nlp(\" is synonymous with\"))\n",
    "matcher_positive.add(\"can_be\", None, nlp(\"can be\"))\n",
    "\n",
    "\n",
    "matcher_positive.add(\"much\", None, nlp(\"much\"), nlp(\"Much\"))\n",
    "matcher_positive.add(\"little\", None, nlp(\"little\"), nlp(\"Little\"))\n",
    "matcher_positive.add(\"more\", None, nlp(\"more\"), nlp(\"More\"))\n",
    "matcher_positive.add(\"less\", None, nlp(\"less\"), nlp(\"Less\"))\n",
    "\n",
    "\n",
    "matcher_positive.add(\"fore_synonym\", None, nlp(\"are synonyms\"), nlp(\"are synonymous\"), nlp(\"is the same thing\"), nlp(\"are the same thing\") )\n",
    "matcher_positive.add(\"then\", None, nlp(\"then\") )\n",
    "matcher_positive.add(\"so\", None, nlp(\"so\") )\n",
    "matcher_positive.add(\"must_be\", None, nlp(\"must be\"))\n",
    "matcher_positive.add(\"hasto\", None, nlp(\"has to be\"))\n",
    "matcher_positive.add(\"is_are\", None, nlp(\"is\"), nlp(\"are\"), nlp(\"Are\"), nlp(\"ARE\"))\n",
    "\n",
    "matcher_positive.add(\"to\", None, nlp(\"to\"))\n",
    "matcher_positive.add(\"To\", None, nlp(\"To\"))\n",
    "matcher_positive.add(\"increase\", None, nlp(\"increase\"), nlp(\"increases\"), nlp(\"Increase\"))\n",
    "\n",
    "matcher_positive.add(\"should\", None, nlp(\"should\"), nlp(\"Should\") )\n",
    "matcher_positive.add(\"would\", None, nlp(\"would\"), nlp(\"Would\"))\n",
    "matcher_positive.add(\"could\", None, nlp(\"could\"), nlp(\"Could\"))\n",
    "matcher_positive.add(\"may\", None, nlp(\"may\"), nlp(\"May\"))\n",
    "matcher_positive.add(\"will\", None, nlp(\"will\"), nlp(\"Will\"))\n",
    "matcher_positive.add(\"can\", None, nlp(\"can\"), nlp(\"Can\"))\n",
    "matcher_positive.add(\"might\", None, nlp(\"might\"), nlp(\"Might\"))\n",
    "matcher_positive.add(\"must\", None, nlp(\"must\"), nlp(\"Must\"), nlp(\"MUST\"))\n",
    "\n",
    "matcher_positive.add(\"encourage\", None, nlp(\"encourage\"), nlp(\"encourages\"), nlp(\"Encourage\"), nlp(\"Encourages\"))\n",
    "\n",
    "matcher_positive.add(\"n’t\", None, nlp(\"n’t\"))\n",
    "matcher_positive.add(\"was_were\", None, nlp(\"was\"), nlp(\"were\"))\n",
    "matcher_positive.add(\"raise\", None, nlp(\"raise\"), nlp(\"raises\"), nlp(\"Raise\"), nlp(\"raising\"), nlp(\"Raising\"))\n",
    "matcher_positive.add(\"better\", None, nlp(\"better\"), nlp(\"Better\"))\n",
    "matcher_positive.add(\"benefit\", None, nlp(\"benefit\"), nlp(\"benefits\"), nlp(\"Benefit\"), nlp(\"Benefits\"))\n",
    "matcher_positive.add(\"lack\", None, nlp(\"lack\"), nlp(\"lacks\"), nlp(\"Lack\"), nlp(\"Lacks\"))\n",
    "matcher_positive.add(\"nothing\", None, nlp(\"nothing\"), nlp(\"Nothing\"))\n",
    "matcher_positive.add(\"positive\", None, nlp(\"positive\"),nlp(\"Positive\"))\n",
    "matcher_positive.add(\"negative\", None, nlp(\"negative\"), nlp(\"Negative\"))\n",
    "matcher_positive.add(\"have\", None, nlp(\"have\"),nlp(\"Have\"))\n",
    "matcher_positive.add(\"has\", None, nlp(\"has\"), nlp(\"Has\"))\n",
    "matcher_positive.add(\"reduce\", None, nlp(\"reduce\"), nlp(\"Reduce\"), nlp(\"reduces\"), nlp(\"Reduces\"), nlp(\"Reduced\"), nlp(\"reduced\"))\n",
    "matcher_positive.add(\"increase\", None, nlp(\"increase\"), nlp(\"Increase\"), nlp(\"increases\"), nlp(\"Increases\"), nlp(\"increased\"), nlp(\"Increased\"))\n",
    "matcher_positive.add(\"without\", None, nlp(\"without\"), nlp(\"Without\"))\n",
    "matcher_positive.add(\"against\", None, nlp(\"against\"), nlp(\"Against\"))\n",
    "matcher_positive.add(\"need\", None, nlp(\"need\"), nlp(\"Need\"), nlp(\"needs\"), nlp(\"Needs\"), nlp(\"needed\"), nlp(\"Needed\"))\n",
    "matcher_positive.add(\"needed\", None, nlp(\"needed\"), nlp(\"Needed\"))\n",
    "matcher_positive.add(\"good\", None, nlp(\"good\"), nlp(\"Good\"))\n",
    "matcher_positive.add(\"bad\", None, nlp(\"bad\"), nlp(\"Bad\"))\n",
    "matcher_positive.add(\"support\", None, nlp(\"support\"), nlp(\"Support\"), nlp(\"supports\"), nlp(\"Supports\"), nlp(\"supported\"), nlp(\"Supported\"))\n",
    "matcher_positive.add(\"hurt_harm_damage\", None, nlp(\"hurt\"), nlp(\"hurts\"), nlp(\"harm\"), nlp(\"harms\"), nlp(\"Hurt\"), nlp(\"Hurts\"), nlp(\"Harm\"), nlp(\"Harms\"), nlp(\"HARM\"), nlp(\"damage\"), nlp(\"damages\"), nlp(\"Damage\"), nlp(\"Damages\"))\n",
    "matcher_positive.add(\"help\", None, nlp(\"help\"), nlp(\"Help\"), nlp(\"helps\"), nlp(\"Helps\"))\n",
    "matcher_positive.add(\"protect\", None, nlp(\"protect\"), nlp(\"Protect\"), nlp(\"protects\"), nlp(\"Protects\"), nlp(\"protecting\"), nlp(\"protected\"), nlp(\"Protecting\"), nlp(\"Protected\"))\n",
    "matcher_positive.add(\"cause\", None, nlp(\"cause\"), nlp(\"Cause\"), nlp(\"causes\"), nlp(\"Causes\"))\n",
    "matcher_positive.add(\"allow\", None, nlp(\"allow\"), nlp(\"Allow\"), nlp(\"allows\"), nlp(\"Allows\"))\n",
    "matcher_positive.add(\"everyone\", None, nlp(\"everyone\"), nlp(\"Everyone\"))\n",
    "matcher_positive.add(\"deserve\", None, nlp(\"deserve\"), nlp(\"Deserve\"), nlp(\"deserves\"), nlp(\"Deserves\"), nlp(\"deserved\"), nlp(\"Deserved\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opposite = []\n",
    "for perspective in column_toprocess:\n",
    "    doc = nlp(perspective)\n",
    "    matches = matcher(doc)\n",
    "    positive_matches = matcher_positive(doc)\n",
    "    if matches:\n",
    "        for match_id, start, end in matches:\n",
    "            rule_id = nlp.vocab.strings[match_id]\n",
    "            if rule_id == \"can't\":\n",
    "                new_seq = str(doc[0:start-1])+\" can \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"not\":\n",
    "                if str(doc[start-1:start]) == \"ca\":\n",
    "                    continue\n",
    "                else:\n",
    "                    new_seq = str(doc[0:start])+\" \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "    elif positive_matches:\n",
    "        for match_id, start, end in positive_matches:\n",
    "            rule_id = nlp.vocab.strings[match_id]\n",
    "            if rule_id == \"type\":\n",
    "                if doc[start:end][0] == \"is\":\n",
    "                    new_seq = str(doc[0:start])+\" is not a type of \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                elif doc[start:end][0] == \"are\":\n",
    "                    new_seq = str(doc[0:start])+\" are not a type of \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"imply\":\n",
    "                if str(doc[start:end][0]) == \"implies\":\n",
    "                    new_seq = str(doc[0:start])+\" does not imply \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                elif str(doc[start:end][0]) == \"imply\":\n",
    "                    new_seq = str(doc[0:start])+\" do not imply \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"same\":\n",
    "                if doc[start:end][0] == \"is\":\n",
    "                    new_seq = str(doc[0:start])+\" is not the same as \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if doc[start:end][0] == \"are\":\n",
    "                    new_seq = str(doc[0:start])+\" are not the same as \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"rephrase\":\n",
    "                new_seq = str(doc[0:start])+\" is not a rephrasing of \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"form\":\n",
    "                new_seq = str(doc[0:start])+\" is a another form of \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"synonym\":\n",
    "                new_seq = str(doc[0:start])+\" is not synonymous with \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"can_be\":\n",
    "                new_seq = str(doc[0:start])+\" can't be \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"fore_synonym\":\n",
    "                new_seq = str(doc[0:start])+\" are not synonymous\"\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"then\":\n",
    "                new_seq = str(doc[0:start])+\" doesn't mean \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"so\":\n",
    "                new_seq = str(doc[0:start])+\" does not mean \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"must_be\":\n",
    "                new_seq = str(doc[0:start])+\" needn't be \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"hasto\":\n",
    "                new_seq = str(doc[0:start])+\" doesn't have to be\"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"much\":\n",
    "                if str(doc[start:end][0]) == \"Much\":\n",
    "                    new_seq = str(\"Little \"+str(doc[end:]))\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"much\":\n",
    "                    new_seq = str(doc[0:start])+\" little \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"little\":\n",
    "                if str(doc[start:end][0]) == \"Little\":\n",
    "                    new_seq = str(\"Much \"+str(doc[end:]))\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"little\":\n",
    "                    new_seq = str(doc[0:start])+\" much \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"more\":\n",
    "                if str(doc[start:end][0]) == \"More\":\n",
    "                    new_seq = str(\"Less \"+str(doc[end:]))\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"more\":\n",
    "                    new_seq = str(doc[0:start])+\" less \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"less\":\n",
    "                if str(doc[start:end][0]) == \"Less\":\n",
    "                    new_seq = str(\"More \"+str(doc[end:]))\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"less\":\n",
    "                    new_seq = str(doc[0:start])+\" more \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"is_are\":\n",
    "                if str(doc[start:end][0]) == \"is\":\n",
    "                    new_seq = str(doc[0:start])+\" is not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"are\":\n",
    "                    new_seq = str(doc[0:start])+\" are not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"Are\":\n",
    "                    new_seq = str(doc[0:start])+\" are not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"ARE\":\n",
    "                    new_seq = str(doc[0:start])+\" are not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"to\":\n",
    "                new_seq = str(doc[0:start])+\" not to \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"To\":\n",
    "                new_seq = str(doc[0:start])+\" Not to \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"increase\":\n",
    "                new_seq = str(doc[0:start])+\" decrease \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"should\":\n",
    "                new_seq = str(doc[0:start])+\" should not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"would\":\n",
    "                new_seq = str(doc[0:start])+\" would not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"could\":\n",
    "                new_seq = str(doc[0:start])+\" could not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"may\":\n",
    "                new_seq = str(doc[0:start])+\" may not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"will\":\n",
    "                new_seq = str(doc[0:start])+\" will not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"can\":\n",
    "                new_seq = str(doc[0:start])+\" can not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"might\":\n",
    "                new_seq = str(doc[0:start])+\" might not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"must\":\n",
    "                new_seq = str(doc[0:start])+\" must not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"encourage\":\n",
    "                new_seq = str(doc[0:start])+\" discourage \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"n’t\":\n",
    "                new_seq = str(doc[0:start])+\" \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"was_were\":\n",
    "                if str(doc[start:end][0]) == \"was\":\n",
    "                    new_seq = str(doc[0:start])+\" was not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"were\":\n",
    "                    new_seq = str(doc[0:start])+\" were not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"raise\":\n",
    "                new_seq = str(doc[0:start])+\" lower \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"better\":\n",
    "                new_seq = str(doc[0:start])+\" worse \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"benefit\":\n",
    "                new_seq = str(doc[0:start])+\" harm \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"lack\":\n",
    "                new_seq = str(doc[0:start])+\" glut \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"nothing\":\n",
    "                new_seq = str(doc[0:start])+\" something \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"positive\":\n",
    "                new_seq = str(doc[0:start])+\" negative \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"negative\":\n",
    "                new_seq = str(doc[0:start])+\" positive \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"have\":\n",
    "                new_seq = str(doc[0:start])+\" don't have \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"has\":\n",
    "                new_seq = str(doc[0:start])+\" doesn't have \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"reduce\":\n",
    "                new_seq = str(doc[0:start])+\" increase \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"increase\":\n",
    "                new_seq = str(doc[0:start])+\" decrease \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"without\":\n",
    "                new_seq = str(doc[0:start])+\" with \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"against\":\n",
    "                new_seq = str(doc[0:start])+\" for \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"needed\":\n",
    "                new_seq = str(doc[0:start])+\" not needed \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"need\":\n",
    "                new_seq = str(doc[0:start])+\" don't need \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"good\":\n",
    "                new_seq = str(doc[0:start])+\" bad \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"bad\":\n",
    "                new_seq = str(doc[0:start])+\" good \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"support\":\n",
    "                new_seq = str(doc[0:start])+\" oppose \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"hurt_harm_damage\":\n",
    "                new_seq = str(doc[0:start])+\" protect \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"help\":\n",
    "                new_seq = str(doc[0:start])+\" spoil \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"protect\":\n",
    "                new_seq = str(doc[0:start])+\" destroy \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"cause\":\n",
    "                new_seq = str(doc[0:start])+\" casue no \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"allow\":\n",
    "                new_seq = str(doc[0:start])+\" disallow \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"everyone\":\n",
    "                new_seq = str(doc[0:start])+\" noone \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"deserve\":\n",
    "                new_seq = str(doc[0:start])+\" deserve no \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "    else:\n",
    "        new_seq = None\n",
    "#         new_seq = str(doc)+\" but it is not true. \"\n",
    "        opposite.append(new_seq)\n",
    "# len(opposite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3478\n",
      "3559\n",
      "0.9772407979769598\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARC train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13878\n",
      "14233\n",
      "0.975057963886742\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval2016 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013\n",
      "1249\n",
      "0.811048839071257\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval2016 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280\n",
      "2814\n",
      "0.8102345415778252\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ibmcs test claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1274\n",
      "1355\n",
      "0.9402214022140222\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ibmcs test perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192\n",
      "1355\n",
      "0.8797047970479704\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite)) \n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ibmcs train claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039\n",
      "1039\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite)) \n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ibmcs train perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927\n",
      "1039\n",
      "0.8922040423484119\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite)) \n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perspectrum dev claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866\n",
      "2096\n",
      "0.8902671755725191\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perspectrum dev perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881\n",
      "2096\n",
      "0.8974236641221374\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stancy critical assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(\"D:/Jupyter/stance_code/Evaluation/bert_dummy_output/non_reverse_bert_cons_epoch5_raw_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undermine_score</th>\n",
       "      <th>support_score</th>\n",
       "      <th>predict_label</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.298447</td>\n",
       "      <td>3.252912</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.847457</td>\n",
       "      <td>-3.262123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.267502</td>\n",
       "      <td>3.174200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.226553</td>\n",
       "      <td>3.147269</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.088458</td>\n",
       "      <td>2.970518</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>2.061727</td>\n",
       "      <td>-1.558688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>1.699087</td>\n",
       "      <td>-1.463090</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>-1.879320</td>\n",
       "      <td>1.803434</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>2.407053</td>\n",
       "      <td>-1.869287</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>-3.018865</td>\n",
       "      <td>2.861231</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2773 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      undermine_score  support_score  predict_label  gold\n",
       "0           -3.298447       3.252912              1     1\n",
       "1            3.847457      -3.262123              0     0\n",
       "2           -3.267502       3.174200              1     1\n",
       "3           -3.226553       3.147269              1     1\n",
       "4           -3.088458       2.970518              1     1\n",
       "...               ...            ...            ...   ...\n",
       "2768         2.061727      -1.558688              0     0\n",
       "2769         1.699087      -1.463090              0     0\n",
       "2770        -1.879320       1.803434              1     1\n",
       "2771         2.407053      -1.869287              0     1\n",
       "2772        -3.018865       2.861231              1     1\n",
       "\n",
       "[2773 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7732441471571906"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_final.gold,df_final.predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761026991441738"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(df_final.gold,df_final.predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7858599592114208"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(df_final.gold,df_final.predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(df_final.gold,df_final.distance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_score(df_final.gold,df_final.distance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall_score(df_final.gold,df_final.distance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.6424792 , 5.56614255, 6.1966165 , 6.44254715, 6.5695213 ,\n",
       "       6.687718  , 6.9646743 , 7.6376425 , 8.10043095])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile = np.percentile(abs(df_final.undermine_score-df_final.support_score), (10,20,30,40,50,60,70,80,90), interpolation='midpoint')\n",
    "percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6376425"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_threshold = df_final[abs(df_final.undermine_score-df_final.support_score)>percentile[8]]\n",
    "# df_final_threshold = df_final[abs(df_final.cp_distance-df_final.cop_distance)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undermine_score</th>\n",
       "      <th>support_score</th>\n",
       "      <th>predict_label</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.393428</td>\n",
       "      <td>-3.880003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.337420</td>\n",
       "      <td>-3.801549</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.393428</td>\n",
       "      <td>-3.880003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.465372</td>\n",
       "      <td>-3.819673</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.407632</td>\n",
       "      <td>-3.795393</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>4.429237</td>\n",
       "      <td>-3.824474</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>4.401361</td>\n",
       "      <td>-3.855954</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>4.366290</td>\n",
       "      <td>-3.819374</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>4.369420</td>\n",
       "      <td>-3.805042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>4.369130</td>\n",
       "      <td>-3.811136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      undermine_score  support_score  predict_label  gold\n",
       "14           4.393428      -3.880003              0     0\n",
       "15           4.337420      -3.801549              0     0\n",
       "17           4.393428      -3.880003              0     0\n",
       "18           4.465372      -3.819673              0     0\n",
       "20           4.407632      -3.795393              0     0\n",
       "...               ...            ...            ...   ...\n",
       "2701         4.429237      -3.824474              0     0\n",
       "2702         4.401361      -3.855954              0     0\n",
       "2718         4.366290      -3.819374              0     0\n",
       "2747         4.369420      -3.805042              0     0\n",
       "2752         4.369130      -3.811136              0     0\n",
       "\n",
       "[278 rows x 4 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f1_score(df_final_threshold.gold,df_final_threshold.distance_label),\n",
    "# precision_score(df_final_threshold.gold,df_final_threshold.distance_label),\n",
    "# recall_score(df_final_threshold.gold,df_final_threshold.distance_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7941397445529677 0.7823834196891192 0.8062547673531655\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8145922746781116 0.8111111111111111 0.8181034482758621\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8290946083418107 0.8463136033229491 0.8125623130608175\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8274950429610046 0.8816901408450705 0.7795765877957659\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7992315081652258 0.906318082788671 0.7147766323024055\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6989247311827957 0.9558823529411765 0.5508474576271186\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014814814814814815 1.0 0.007462686567164179 0.8401442307692307\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "accuracy_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0 0.8738738738738738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\arsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "accuracy_score(df_final_threshold.gold,df_final_threshold.predict_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0 0.9316546762589928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\arsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "     accuracy_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

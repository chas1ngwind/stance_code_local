{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/ARC/arc_stances_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/ARC/arc_stances_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/FNC-1/competition_test_stances.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/FNC-1/train_stances.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/SemEval2016Task6/testdata-taskA-all-annotations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"../mdl-stance-robustness/data/SemEval2016Task6/trainingdata-all-annotations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"D:/Jupyter/data/dataset/ibmcs/test_ori.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = \"D:/Jupyter/data/dataset/ibmcs/test_train.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"D:/Jupyter/stance_code/Dataset/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(test_file)\n",
    "# df = pd.read_table(test_file,encoding='ISO-8859-1')\n",
    "df = pd.read_csv(test_file,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>claim</th>\n",
       "      <th>perspective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School Day Should Be Extended</td>\n",
       "      <td>Lessons would feel less broken.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School Day Should Be Extended</td>\n",
       "      <td>Not enough fundng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School Day Should Be Extended</td>\n",
       "      <td>So much easier for parents!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School Day Should Be Extended</td>\n",
       "      <td>Less need for homework.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School Day Should Be Extended</td>\n",
       "      <td>Time to finish activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Illegal downloaders should be cut off from the...</td>\n",
       "      <td>Cutting off internet capability is against hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Illegal downloaders should be cut off from the...</td>\n",
       "      <td>All humans deserve internet access.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Illegal downloaders should be cut off from the...</td>\n",
       "      <td>Internet access is a commodity not a human right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Illegal downloaders should be cut off from the...</td>\n",
       "      <td>The internet is a utility not a human right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Illegal downloaders should be cut off from the...</td>\n",
       "      <td>Just as electricity is not a human right, the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2773 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label  x  y                                              claim  \\\n",
       "0         1  0  0                      School Day Should Be Extended   \n",
       "1         0  0  0                      School Day Should Be Extended   \n",
       "2         1  0  0                      School Day Should Be Extended   \n",
       "3         1  0  0                      School Day Should Be Extended   \n",
       "4         1  0  0                      School Day Should Be Extended   \n",
       "...     ... .. ..                                                ...   \n",
       "2768      0  0  0  Illegal downloaders should be cut off from the...   \n",
       "2769      0  0  0  Illegal downloaders should be cut off from the...   \n",
       "2770      1  0  0  Illegal downloaders should be cut off from the...   \n",
       "2771      1  0  0  Illegal downloaders should be cut off from the...   \n",
       "2772      1  0  0  Illegal downloaders should be cut off from the...   \n",
       "\n",
       "                                            perspective  \n",
       "0                       Lessons would feel less broken.  \n",
       "1                                  Not enough fundng...  \n",
       "2                           So much easier for parents!  \n",
       "3                               Less need for homework.  \n",
       "4                             Time to finish activities  \n",
       "...                                                 ...  \n",
       "2768  Cutting off internet capability is against hum...  \n",
       "2769                All humans deserve internet access.  \n",
       "2770  Internet access is a commodity not a human right.  \n",
       "2771       The internet is a utility not a human right.  \n",
       "2772  Just as electricity is not a human right, the ...  \n",
       "\n",
       "[2773 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_toprocess = df.Headline\n",
    "column_toprocess = df.perspective\n",
    "# column_toprocess = df.claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fucntion to auto create opposite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"lower\")\n",
    "matcher.add(\"not\", None, nlp(\"not\"), nlp(\"n't\"))\n",
    "matcher.add(\"can't\", None, nlp(\"can't\"))\n",
    "\n",
    "\n",
    "matcher_positive = PhraseMatcher(nlp.vocab, attr=\"ORTH\")\n",
    "matcher_positive.add(\"type\", None, nlp(\"is a type of\"), nlp(\"are a type of\") )\n",
    "matcher_positive.add(\"imply\", None, nlp(\"implies\"), nlp(\"imply\") )\n",
    "matcher_positive.add(\"same\", None, nlp(\"is the same as \"), nlp(\"are the same as \") )\n",
    "matcher_positive.add(\"rephrase\", None, nlp(\" is a rephrasing of\") )\n",
    "matcher_positive.add(\"form\", None, nlp(\"is a another form of\"))\n",
    "matcher_positive.add(\"synonym\", None, nlp(\" is synonymous with\"))\n",
    "matcher_positive.add(\"can_be\", None, nlp(\"can be\"))\n",
    "\n",
    "\n",
    "matcher_positive.add(\"much\", None, nlp(\"much\"), nlp(\"Much\"))\n",
    "matcher_positive.add(\"little\", None, nlp(\"little\"), nlp(\"Little\"))\n",
    "matcher_positive.add(\"more\", None, nlp(\"more\"), nlp(\"More\"))\n",
    "matcher_positive.add(\"less\", None, nlp(\"less\"), nlp(\"Less\"))\n",
    "\n",
    "\n",
    "matcher_positive.add(\"fore_synonym\", None, nlp(\"are synonyms\"), nlp(\"are synonymous\"), nlp(\"is the same thing\"), nlp(\"are the same thing\") )\n",
    "matcher_positive.add(\"then\", None, nlp(\"then\") )\n",
    "matcher_positive.add(\"so\", None, nlp(\"so\") )\n",
    "matcher_positive.add(\"must_be\", None, nlp(\"must be\"))\n",
    "matcher_positive.add(\"hasto\", None, nlp(\"has to be\"))\n",
    "matcher_positive.add(\"is_are\", None, nlp(\"is\"), nlp(\"are\"), nlp(\"Are\"), nlp(\"ARE\"))\n",
    "\n",
    "matcher_positive.add(\"to\", None, nlp(\"to\"))\n",
    "matcher_positive.add(\"To\", None, nlp(\"To\"))\n",
    "matcher_positive.add(\"increase\", None, nlp(\"increase\"), nlp(\"increases\"), nlp(\"Increase\"))\n",
    "\n",
    "matcher_positive.add(\"should\", None, nlp(\"should\"), nlp(\"Should\") )\n",
    "matcher_positive.add(\"would\", None, nlp(\"would\"), nlp(\"Would\"))\n",
    "matcher_positive.add(\"could\", None, nlp(\"could\"), nlp(\"Could\"))\n",
    "matcher_positive.add(\"may\", None, nlp(\"may\"), nlp(\"May\"))\n",
    "matcher_positive.add(\"will\", None, nlp(\"will\"), nlp(\"Will\"))\n",
    "matcher_positive.add(\"can\", None, nlp(\"can\"), nlp(\"Can\"))\n",
    "matcher_positive.add(\"might\", None, nlp(\"might\"), nlp(\"Might\"))\n",
    "matcher_positive.add(\"must\", None, nlp(\"must\"), nlp(\"Must\"), nlp(\"MUST\"))\n",
    "\n",
    "matcher_positive.add(\"encourage\", None, nlp(\"encourage\"), nlp(\"encourages\"), nlp(\"Encourage\"), nlp(\"Encourages\"))\n",
    "\n",
    "matcher_positive.add(\"n’t\", None, nlp(\"n’t\"))\n",
    "matcher_positive.add(\"was_were\", None, nlp(\"was\"), nlp(\"were\"))\n",
    "matcher_positive.add(\"raise\", None, nlp(\"raise\"), nlp(\"raises\"), nlp(\"Raise\"), nlp(\"raising\"), nlp(\"Raising\"))\n",
    "matcher_positive.add(\"better\", None, nlp(\"better\"), nlp(\"Better\"))\n",
    "matcher_positive.add(\"benefit\", None, nlp(\"benefit\"), nlp(\"benefits\"), nlp(\"Benefit\"), nlp(\"Benefits\"))\n",
    "matcher_positive.add(\"lack\", None, nlp(\"lack\"), nlp(\"lacks\"), nlp(\"Lack\"), nlp(\"Lacks\"))\n",
    "matcher_positive.add(\"nothing\", None, nlp(\"nothing\"), nlp(\"Nothing\"))\n",
    "matcher_positive.add(\"positive\", None, nlp(\"positive\"),nlp(\"Positive\"))\n",
    "matcher_positive.add(\"negative\", None, nlp(\"negative\"), nlp(\"Negative\"))\n",
    "matcher_positive.add(\"have\", None, nlp(\"have\"),nlp(\"Have\"))\n",
    "matcher_positive.add(\"has\", None, nlp(\"has\"), nlp(\"Has\"))\n",
    "matcher_positive.add(\"reduce\", None, nlp(\"reduce\"), nlp(\"Reduce\"), nlp(\"reduces\"), nlp(\"Reduces\"), nlp(\"Reduced\"), nlp(\"reduced\"))\n",
    "matcher_positive.add(\"increase\", None, nlp(\"increase\"), nlp(\"Increase\"), nlp(\"increases\"), nlp(\"Increases\"), nlp(\"increased\"), nlp(\"Increased\"))\n",
    "matcher_positive.add(\"without\", None, nlp(\"without\"), nlp(\"Without\"))\n",
    "matcher_positive.add(\"against\", None, nlp(\"against\"), nlp(\"Against\"))\n",
    "matcher_positive.add(\"need\", None, nlp(\"need\"), nlp(\"Need\"), nlp(\"needs\"), nlp(\"Needs\"), nlp(\"needed\"), nlp(\"Needed\"))\n",
    "matcher_positive.add(\"needed\", None, nlp(\"needed\"), nlp(\"Needed\"))\n",
    "matcher_positive.add(\"good\", None, nlp(\"good\"), nlp(\"Good\"))\n",
    "matcher_positive.add(\"bad\", None, nlp(\"bad\"), nlp(\"Bad\"))\n",
    "matcher_positive.add(\"support\", None, nlp(\"support\"), nlp(\"Support\"), nlp(\"supports\"), nlp(\"Supports\"), nlp(\"supported\"), nlp(\"Supported\"))\n",
    "matcher_positive.add(\"hurt_harm_damage\", None, nlp(\"hurt\"), nlp(\"hurts\"), nlp(\"harm\"), nlp(\"harms\"), nlp(\"Hurt\"), nlp(\"Hurts\"), nlp(\"Harm\"), nlp(\"Harms\"), nlp(\"HARM\"), nlp(\"damage\"), nlp(\"damages\"), nlp(\"Damage\"), nlp(\"Damages\"))\n",
    "matcher_positive.add(\"help\", None, nlp(\"help\"), nlp(\"Help\"), nlp(\"helps\"), nlp(\"Helps\"))\n",
    "matcher_positive.add(\"protect\", None, nlp(\"protect\"), nlp(\"Protect\"), nlp(\"protects\"), nlp(\"Protects\"), nlp(\"protecting\"), nlp(\"protected\"), nlp(\"Protecting\"), nlp(\"Protected\"))\n",
    "matcher_positive.add(\"cause\", None, nlp(\"cause\"), nlp(\"Cause\"), nlp(\"causes\"), nlp(\"Causes\"))\n",
    "matcher_positive.add(\"allow\", None, nlp(\"allow\"), nlp(\"Allow\"), nlp(\"allows\"), nlp(\"Allows\"))\n",
    "matcher_positive.add(\"everyone\", None, nlp(\"everyone\"), nlp(\"Everyone\"))\n",
    "matcher_positive.add(\"deserve\", None, nlp(\"deserve\"), nlp(\"Deserve\"), nlp(\"deserves\"), nlp(\"Deserves\"), nlp(\"deserved\"), nlp(\"Deserved\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opposite = []\n",
    "for perspective in column_toprocess:\n",
    "    doc = nlp(perspective)\n",
    "    matches = matcher(doc)\n",
    "    positive_matches = matcher_positive(doc)\n",
    "    if matches:\n",
    "        for match_id, start, end in matches:\n",
    "            rule_id = nlp.vocab.strings[match_id]\n",
    "            if rule_id == \"can't\":\n",
    "                new_seq = str(doc[0:start-1])+\" can \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"not\":\n",
    "                if str(doc[start-1:start]) == \"ca\":\n",
    "                    continue\n",
    "                else:\n",
    "                    new_seq = str(doc[0:start])+\" \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "    elif positive_matches:\n",
    "        for match_id, start, end in positive_matches:\n",
    "            rule_id = nlp.vocab.strings[match_id]\n",
    "            if rule_id == \"type\":\n",
    "                if doc[start:end][0] == \"is\":\n",
    "                    new_seq = str(doc[0:start])+\" is not a type of \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                elif doc[start:end][0] == \"are\":\n",
    "                    new_seq = str(doc[0:start])+\" are not a type of \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"imply\":\n",
    "                if str(doc[start:end][0]) == \"implies\":\n",
    "                    new_seq = str(doc[0:start])+\" does not imply \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                elif str(doc[start:end][0]) == \"imply\":\n",
    "                    new_seq = str(doc[0:start])+\" do not imply \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"same\":\n",
    "                if doc[start:end][0] == \"is\":\n",
    "                    new_seq = str(doc[0:start])+\" is not the same as \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if doc[start:end][0] == \"are\":\n",
    "                    new_seq = str(doc[0:start])+\" are not the same as \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"rephrase\":\n",
    "                new_seq = str(doc[0:start])+\" is not a rephrasing of \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"form\":\n",
    "                new_seq = str(doc[0:start])+\" is a another form of \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"synonym\":\n",
    "                new_seq = str(doc[0:start])+\" is not synonymous with \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"can_be\":\n",
    "                new_seq = str(doc[0:start])+\" can't be \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"fore_synonym\":\n",
    "                new_seq = str(doc[0:start])+\" are not synonymous\"\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"then\":\n",
    "                new_seq = str(doc[0:start])+\" doesn't mean \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"so\":\n",
    "                new_seq = str(doc[0:start])+\" does not mean \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"must_be\":\n",
    "                new_seq = str(doc[0:start])+\" needn't be \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break \n",
    "            elif rule_id == \"hasto\":\n",
    "                new_seq = str(doc[0:start])+\" doesn't have to be\"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"much\":\n",
    "                if str(doc[start:end][0]) == \"Much\":\n",
    "                    new_seq = str(\"Little \"+str(doc[end:]))\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"much\":\n",
    "                    new_seq = str(doc[0:start])+\" little \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"little\":\n",
    "                if str(doc[start:end][0]) == \"Little\":\n",
    "                    new_seq = str(\"Much \"+str(doc[end:]))\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"little\":\n",
    "                    new_seq = str(doc[0:start])+\" much \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"more\":\n",
    "                if str(doc[start:end][0]) == \"More\":\n",
    "                    new_seq = str(\"Less \"+str(doc[end:]))\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"more\":\n",
    "                    new_seq = str(doc[0:start])+\" less \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"less\":\n",
    "                if str(doc[start:end][0]) == \"Less\":\n",
    "                    new_seq = str(\"More \"+str(doc[end:]))\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"less\":\n",
    "                    new_seq = str(doc[0:start])+\" more \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"is_are\":\n",
    "                if str(doc[start:end][0]) == \"is\":\n",
    "                    new_seq = str(doc[0:start])+\" is not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"are\":\n",
    "                    new_seq = str(doc[0:start])+\" are not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"Are\":\n",
    "                    new_seq = str(doc[0:start])+\" are not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"ARE\":\n",
    "                    new_seq = str(doc[0:start])+\" are not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"to\":\n",
    "                new_seq = str(doc[0:start])+\" not to \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"To\":\n",
    "                new_seq = str(doc[0:start])+\" Not to \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"increase\":\n",
    "                new_seq = str(doc[0:start])+\" decrease \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"should\":\n",
    "                new_seq = str(doc[0:start])+\" should not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"would\":\n",
    "                new_seq = str(doc[0:start])+\" would not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"could\":\n",
    "                new_seq = str(doc[0:start])+\" could not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"may\":\n",
    "                new_seq = str(doc[0:start])+\" may not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"will\":\n",
    "                new_seq = str(doc[0:start])+\" will not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"can\":\n",
    "                new_seq = str(doc[0:start])+\" can not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"might\":\n",
    "                new_seq = str(doc[0:start])+\" might not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"must\":\n",
    "                new_seq = str(doc[0:start])+\" must not \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"encourage\":\n",
    "                new_seq = str(doc[0:start])+\" discourage \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"n’t\":\n",
    "                new_seq = str(doc[0:start])+\" \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"was_were\":\n",
    "                if str(doc[start:end][0]) == \"was\":\n",
    "                    new_seq = str(doc[0:start])+\" was not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "                if str(doc[start:end][0]) == \"were\":\n",
    "                    new_seq = str(doc[0:start])+\" were not \"+str(doc[end:])\n",
    "                    opposite.append(new_seq)\n",
    "                    break\n",
    "            elif rule_id == \"raise\":\n",
    "                new_seq = str(doc[0:start])+\" lower \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"better\":\n",
    "                new_seq = str(doc[0:start])+\" worse \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"benefit\":\n",
    "                new_seq = str(doc[0:start])+\" harm \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"lack\":\n",
    "                new_seq = str(doc[0:start])+\" glut \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"nothing\":\n",
    "                new_seq = str(doc[0:start])+\" something \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"positive\":\n",
    "                new_seq = str(doc[0:start])+\" negative \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"negative\":\n",
    "                new_seq = str(doc[0:start])+\" positive \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"have\":\n",
    "                new_seq = str(doc[0:start])+\" don't have \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"has\":\n",
    "                new_seq = str(doc[0:start])+\" doesn't have \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"reduce\":\n",
    "                new_seq = str(doc[0:start])+\" increase \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"increase\":\n",
    "                new_seq = str(doc[0:start])+\" decrease \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"without\":\n",
    "                new_seq = str(doc[0:start])+\" with \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"against\":\n",
    "                new_seq = str(doc[0:start])+\" for \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"needed\":\n",
    "                new_seq = str(doc[0:start])+\" not needed \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"need\":\n",
    "                new_seq = str(doc[0:start])+\" don't need \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"good\":\n",
    "                new_seq = str(doc[0:start])+\" bad \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"bad\":\n",
    "                new_seq = str(doc[0:start])+\" good \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"support\":\n",
    "                new_seq = str(doc[0:start])+\" oppose \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"hurt_harm_damage\":\n",
    "                new_seq = str(doc[0:start])+\" protect \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"help\":\n",
    "                new_seq = str(doc[0:start])+\" spoil \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"protect\":\n",
    "                new_seq = str(doc[0:start])+\" destroy \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"cause\":\n",
    "                new_seq = str(doc[0:start])+\" casue no \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"allow\":\n",
    "                new_seq = str(doc[0:start])+\" disallow \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"everyone\":\n",
    "                new_seq = str(doc[0:start])+\" noone \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "            elif rule_id == \"deserve\":\n",
    "                new_seq = str(doc[0:start])+\" deserve no \"+str(doc[end:])\n",
    "                opposite.append(new_seq)\n",
    "                break\n",
    "    else:\n",
    "        new_seq = None\n",
    "#         new_seq = str(doc)+\" but it is not true. \"\n",
    "        opposite.append(new_seq)\n",
    "# len(opposite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3478\n",
      "3559\n",
      "0.9772407979769598\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARC train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13878\n",
      "14233\n",
      "0.975057963886742\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval2016 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013\n",
      "1249\n",
      "0.811048839071257\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval2016 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280\n",
      "2814\n",
      "0.8102345415778252\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ibmcs test claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1274\n",
      "1355\n",
      "0.9402214022140222\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ibmcs test perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192\n",
      "1355\n",
      "0.8797047970479704\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite)) \n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ibmcs train claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039\n",
      "1039\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite)) \n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ibmcs train perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927\n",
      "1039\n",
      "0.8922040423484119\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite)) \n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perspectrum dev claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866\n",
      "2096\n",
      "0.8902671755725191\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perspectrum dev perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881\n",
      "2096\n",
      "0.8974236641221374\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(None,opposite))))\n",
    "print(len(opposite))\n",
    "print(len(list(filter(None,opposite)))/len(opposite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stancy critical assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(\"D:/Jupyter/stance_code/Evaluation/bert_dummy_output/bert_base_epoch5_raw_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undermine_score</th>\n",
       "      <th>support_score</th>\n",
       "      <th>predict_label</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.406277</td>\n",
       "      <td>1.681480</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.568123</td>\n",
       "      <td>2.129677</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.825778</td>\n",
       "      <td>3.542665</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.605859</td>\n",
       "      <td>1.971612</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.932156</td>\n",
       "      <td>2.379100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>-0.478038</td>\n",
       "      <td>0.624620</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>-2.317768</td>\n",
       "      <td>2.754284</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>4.150797</td>\n",
       "      <td>-4.363125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>4.253913</td>\n",
       "      <td>-4.182820</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>1.890408</td>\n",
       "      <td>-1.864657</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2773 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      undermine_score  support_score  predict_label  gold\n",
       "0           -1.406277       1.681480              1     1\n",
       "1           -1.568123       2.129677              1     0\n",
       "2           -2.825778       3.542665              1     1\n",
       "3           -1.605859       1.971612              1     1\n",
       "4           -1.932156       2.379100              1     1\n",
       "...               ...            ...            ...   ...\n",
       "2768        -0.478038       0.624620              1     0\n",
       "2769        -2.317768       2.754284              1     0\n",
       "2770         4.150797      -4.363125              0     1\n",
       "2771         4.253913      -4.182820              0     1\n",
       "2772         1.890408      -1.864657              0     1\n",
       "\n",
       "[2773 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6962411107348458"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_final.gold,df_final.predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6936572199730094"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(df_final.gold,df_final.predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.698844323589395"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(df_final.gold,df_final.predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(df_final.gold,df_final.distance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_score(df_final.gold,df_final.distance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall_score(df_final.gold,df_final.distance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.41072685, 2.72655345, 3.9238805 , 4.94601505, 5.8205108 ,\n",
       "       6.468594  , 6.9764602 , 7.3613918 , 7.84327645])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile = np.percentile(abs(df_final.undermine_score-df_final.support_score), (10,20,30,40,50,60,70,80,90), interpolation='midpoint')\n",
    "percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.3613918"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_threshold = df_final[abs(df_final.undermine_score-df_final.support_score)>percentile[8]]\n",
    "# df_final_threshold = df_final[abs(df_final.cp_distance-df_final.cop_distance)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undermine_score</th>\n",
       "      <th>support_score</th>\n",
       "      <th>predict_label</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.406277</td>\n",
       "      <td>1.681480</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.568123</td>\n",
       "      <td>2.129677</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.825778</td>\n",
       "      <td>3.542665</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.605859</td>\n",
       "      <td>1.971612</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.932156</td>\n",
       "      <td>2.379100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>2.430949</td>\n",
       "      <td>-2.300661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>-2.317768</td>\n",
       "      <td>2.754284</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>4.150797</td>\n",
       "      <td>-4.363125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>4.253913</td>\n",
       "      <td>-4.182820</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>1.890408</td>\n",
       "      <td>-1.864657</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2495 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      undermine_score  support_score  predict_label  gold\n",
       "0           -1.406277       1.681480              1     1\n",
       "1           -1.568123       2.129677              1     0\n",
       "2           -2.825778       3.542665              1     1\n",
       "3           -1.605859       1.971612              1     1\n",
       "4           -1.932156       2.379100              1     1\n",
       "...               ...            ...            ...   ...\n",
       "2767         2.430949      -2.300661              0     0\n",
       "2769        -2.317768       2.754284              1     0\n",
       "2770         4.150797      -4.363125              0     1\n",
       "2771         4.253913      -4.182820              0     1\n",
       "2772         1.890408      -1.864657              0     1\n",
       "\n",
       "[2495 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f1_score(df_final_threshold.gold,df_final_threshold.distance_label),\n",
    "# precision_score(df_final_threshold.gold,df_final_threshold.distance_label),\n",
    "# recall_score(df_final_threshold.gold,df_final_threshold.distance_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7194029850746269 0.724267468069121 0.7146034099332839\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7398066414459857 0.7470288624787776 0.7327227310574521\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764367816091954 0.7785365853658537 0.7507055503292568\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7839819514946419 0.8081395348837209 0.7612267250821467\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8060731538992408 0.8390804597701149 0.7755644090305445\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8150873965041399 0.878968253968254 0.7598627787307033\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7789165446559299 0.8721311475409836 0.7037037037037037 0.8185096153846154\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "accuracy_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6710097719869706 0.8803418803418803 0.5421052631578948 0.818018018018018\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "accuracy_score(df_final_threshold.gold,df_final_threshold.predict_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07142857142857142 0.6666666666666666 0.03773584905660377 0.8129496402877698\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "precision_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "recall_score(df_final_threshold.gold,df_final_threshold.predict_label),\n",
    "     accuracy_score(df_final_threshold.gold,df_final_threshold.predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
